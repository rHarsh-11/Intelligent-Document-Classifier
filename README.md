
# ğŸ“„ Intelligent Document Classifier (20 Newsgroups NLP Project)

An **NLP-powered document classification system** that automatically categorizes text into one of 20 predefined topics using **TF-IDF** and **Logistic Regression**.  
The project uses the **20 Newsgroups dataset** and includes a **Streamlit web app** for real-time classification.

---

## ğŸš€ Features
- **Text Preprocessing** (lowercasing, tokenization, stopword removal)
- **TF-IDF Vectorization** for feature extraction
- **Multi-class Logistic Regression** for classification
- **Evaluation Metrics**: Precision, Recall, F1-score
- **Confusion Matrix Visualization**
- **Streamlit UI** for interactive predictions
- **Custom Input Prediction** for any text

---

## ğŸ›  Tech Stack
- **Language:** Python
- **Libraries:**
  - `scikit-learn` (ML & evaluation)
  - `nltk` (tokenization, stopwords)
  - `pandas`, `numpy` (data handling)
  - `matplotlib`, `seaborn` (visualization)
  - `streamlit` (web UI)
  - `joblib` (model saving/loading)

---

## ğŸ“‚ Project Structure
```

intelligent\_document\_classifier/
â”‚â”€â”€ models/
â”‚   â”œâ”€â”€ document\_classifier\_model.pkl
â”‚   â”œâ”€â”€ tfidf\_vectorizer.pkl
â”‚â”€â”€ notebooks/
â”‚   â”œâ”€â”€ Regression_model.ipynb
â”‚   â”œâ”€â”€ Model_test.ipynb
â”‚â”€â”€ app.py                  # Streamlit application
â”‚â”€â”€ requirements.txt
â”‚â”€â”€ .gitignore
â”‚â”€â”€ README.md

````

---

## ğŸ“Š Dataset
We use the **[20 Newsgroups dataset](https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html)** from Scikit-learn,  
which contains ~18,000 documents across 20 categories:
- Religion, Politics, Science, Sports, Computers, and more.

---

## âš™ï¸ Installation & Setup

### 1ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/rHarsh-11/Intelligent-Document-Classifier
cd Intelligent-Document-Classifier
````

### 2ï¸âƒ£ Create & Activate Virtual Environment

```bash
python -m venv .venv
source .venv/Scripts/activate
```

### 3ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ Download NLTK Data

```bash
python -m nltk.downloader stopwords punkt
```

---

## â–¶ï¸ Running the Jupyter Notebook

```bash
jupyter notebook notebooks/document_classifier.ipynb
```

---

## ğŸŒ Running the Streamlit App

```bash
streamlit run app.py
```

Then open **[http://localhost:8501](http://localhost:8501)** in your browser.

---

## ğŸ“ˆ Example Output

* **Prediction:**

```
Predicted Category: rec.sport.hockey
```

---

Do you want me to also create the **`requirements.txt`** right now so that once you push this with README, the Streamlit Cloud deployment will work without missing dependencies? Thatâ€™s the next logical step.
```
